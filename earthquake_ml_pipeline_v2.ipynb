{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Earthquake ML Pipeline (Colab)\n",
        "This notebook runs the full v2 pipeline: dataset generation, scaling, model training, and prediction.\n",
        "\n",
        "**Expected data format**: each earthquake `.txt` file contains two columns: time `t` and acceleration `ag`.\n",
        "\n",
        "If you upload files via the Colab file browser, place `.txt` files into `/content/project/earthquake_data`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies (Colab usually has these, but this avoids version errors).\n",
        "!pip -q install -U 'tensorflow>=2.12,<3' scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "print('TensorFlow:', tf.__version__)\n",
        "print('NumPy:', np.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Project structure\n",
        "PROJECT_PATH = '/content/project'\n",
        "EQ_FOLDER = os.path.join(PROJECT_PATH, 'earthquake_data')\n",
        "DATASET_FOLDER = os.path.join(PROJECT_PATH, 'dataset_v2')\n",
        "MODEL_FOLDER = os.path.join(PROJECT_PATH, 'models_v2')\n",
        "\n",
        "os.makedirs(EQ_FOLDER, exist_ok=True)\n",
        "os.makedirs(DATASET_FOLDER, exist_ok=True)\n",
        "os.makedirs(MODEL_FOLDER, exist_ok=True)\n",
        "\n",
        "print('EQ_FOLDER:', EQ_FOLDER)\n",
        "print('DATASET_FOLDER:', DATASET_FOLDER)\n",
        "print('MODEL_FOLDER:', MODEL_FOLDER)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: upload earthquake .txt files into EQ_FOLDER\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for name in uploaded.keys():\n",
        "    if name.lower().endswith('.txt'):\n",
        "        os.replace(name, os.path.join(EQ_FOLDER, name))\n",
        "print('Files in EQ_FOLDER:', os.listdir(EQ_FOLDER))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Generate dataset (v2)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CONFIG\n",
        "FLOOR_LIST = [5, 10, 20, 30]\n",
        "ALPHA_LIST = [0.05, 0.06, 0.065]\n",
        "W_LIST = [5000, 8000, 12000, 16000]  # total building weight [kN]\n",
        "SCALE_LIST = [0.5, 0.75, 1.0, 1.25, 1.5, 1.67, 1.75]\n",
        "\n",
        "h = 0.02\n",
        "g = 980.0\n",
        "T0 = 4.0\n",
        "\n",
        "# lead plug parameter (theory)\n",
        "Dp = 354.0  # mm\n",
        "\n",
        "def trapezoid(y, dx):\n",
        "    # numpy compatibility: np.trapezoid is preferred, np.trapz is fallback\n",
        "    return np.trapezoid(y, dx=dx) if hasattr(np, 'trapezoid') else np.trapz(y, dx=dx)\n",
        "\n",
        "def calc_r(Dp):\n",
        "    return 0.001214 * Dp + 0.5698 if Dp <= 354.0 else 1.0\n",
        "\n",
        "def calc_rho(E_Nmm, Vp, Dp):\n",
        "    r = calc_r(Dp)\n",
        "    E_over_Vp = E_Nmm / Vp\n",
        "    E_eff_over_Vp = r * E_over_Vp\n",
        "    rho = (8.33 / 7.967) * (-0.06 + 1.25 * np.exp(-E_eff_over_Vp / 360.0))\n",
        "    return min(rho, 1.0)\n",
        "\n",
        "def extract_features(t, ag):\n",
        "    dt = t[1] - t[0]\n",
        "    PGA = np.max(np.abs(ag))\n",
        "    PGV = np.max(np.abs(np.cumsum(ag) * dt))\n",
        "    PGD = np.max(np.abs(np.cumsum(np.cumsum(ag) * dt) * dt))\n",
        "    RMS = np.sqrt(np.mean(ag ** 2))\n",
        "    IA = trapezoid(ag ** 2, dx=dt)\n",
        "    CAV = np.sum(np.abs(ag)) * dt\n",
        "    duration = t[-1] - t[0]\n",
        "    return np.array([PGA, PGV, PGD, RMS, IA, CAV, duration], dtype=float)\n",
        "\n",
        "def run_analysis(W_total, N_story, alpha_lp, t, ag, h):\n",
        "    N = N_story + 1\n",
        "    m = W_total / g\n",
        "    omega0 = 2 * np.pi / T0\n",
        "\n",
        "    steps = len(t)\n",
        "    dt = t[1] - t[0]\n",
        "\n",
        "    # Upper structure stiffness\n",
        "    def calc_upper_structure():\n",
        "        T1 = N_story / 10\n",
        "        Wi = np.full(N_story, W_total / N_story)\n",
        "        mi = W_total / g\n",
        "        alpha = np.array([np.sum(Wi[i:]) / np.sum(Wi) for i in range(N_story)])\n",
        "        Ai = 1 + ((1 / np.sqrt(alpha) - alpha) * (2 * T1) / (1 + 3 * T1))\n",
        "        Aalpha = Ai * alpha\n",
        "        k_prime = np.zeros((N_story, N_story))\n",
        "        for i in range(N_story):\n",
        "            k_prime[i, i] = Aalpha[i]\n",
        "            if i < N_story - 1:\n",
        "                k_prime[i, i + 1] = -Aalpha[i + 1]\n",
        "        Jinv = np.eye(N_story) - np.eye(N_story, k=-1)\n",
        "        M = Jinv @ k_prime\n",
        "        eig = np.sort(np.real(np.linalg.eigvals(M)))\n",
        "        omega = np.sqrt(eig[eig > 0])\n",
        "        k1 = mi * (2 * np.pi / T1) ** 2 / omega[0] ** 2\n",
        "        return k1 * Aalpha\n",
        "\n",
        "    k_story = calc_upper_structure()\n",
        "\n",
        "    # Find isolator stiffness kI\n",
        "    omega_target = 2 * np.pi / T0\n",
        "\n",
        "    def omega1_sq(KI):\n",
        "        k = np.concatenate(([KI], k_story))\n",
        "        K = np.zeros((N, N))\n",
        "        for i in range(N):\n",
        "            K[i, i] = k[i]\n",
        "            if i < N - 1:\n",
        "                K[i, i + 1] = -k[i + 1]\n",
        "        Jinv = np.eye(N) - np.eye(N, k=-1)\n",
        "        M = (Jinv @ K) / m\n",
        "        eig = np.sort(np.real(np.linalg.eigvals(M)))\n",
        "        return eig[0]\n",
        "\n",
        "    lo, hi = 0.0, 1e6\n",
        "    mid = 0.0\n",
        "    for _ in range(200):\n",
        "        mid = 0.5 * (lo + hi)\n",
        "        if omega1_sq(mid) > omega_target ** 2:\n",
        "            hi = mid\n",
        "        else:\n",
        "            lo = mid\n",
        "\n",
        "    kI = mid\n",
        "    kd = 10 * kI\n",
        "\n",
        "    # Matrices\n",
        "    Jinv = np.eye(N)\n",
        "    for i in range(1, N):\n",
        "        Jinv[i, i - 1] = -1.0\n",
        "    J = np.linalg.inv(Jinv)\n",
        "\n",
        "    K = np.zeros((N, N))\n",
        "    K[0, 0] = kI\n",
        "    K[0, 1] = -k_story[0]\n",
        "    for i in range(1, N - 1):\n",
        "        K[i, i] = k_story[i - 1]\n",
        "        K[i, i + 1] = -k_story[i]\n",
        "    K[-1, -1] = k_story[-1]\n",
        "\n",
        "    C_story = (2 * h / omega0) * k_story\n",
        "    C = np.zeros((N, N))\n",
        "    C[0, 1] = -C_story[0]\n",
        "    for i in range(1, N - 1):\n",
        "        C[i, i] = C_story[i - 1]\n",
        "        C[i, i + 1] = -C_story[i]\n",
        "    C[-1, -1] = C_story[-1]\n",
        "\n",
        "    # Slip model\n",
        "    delta = alpha_lp * N * W_total / kd\n",
        "\n",
        "    def slip(y, v):\n",
        "        return 0.25 * v * (\n",
        "            2 + np.sign(y + delta) - np.sign(y - delta)\n",
        "            - np.sign(v) * (np.sign(y + delta) + np.sign(y - delta))\n",
        "        )\n",
        "\n",
        "    one = np.zeros(N)\n",
        "    one[0] = 1.0\n",
        "\n",
        "    def accel(u, v, y, agi):\n",
        "        ky = np.zeros(N)\n",
        "        ky[0] = kd * y[0]\n",
        "        rhs = C @ v + K @ u + ky\n",
        "        return -Jinv @ (rhs / m) - one * agi\n",
        "\n",
        "    # Time integration (RK2)\n",
        "    u = np.zeros((N, steps))\n",
        "    v = np.zeros((N, steps))\n",
        "    y = np.zeros((N, steps))\n",
        "\n",
        "    for i in range(steps - 1):\n",
        "        a1 = accel(u[:, i], v[:, i], y[:, i], ag[i])\n",
        "        u2 = u[:, i] + 0.5 * dt * v[:, i]\n",
        "        v2 = v[:, i] + 0.5 * dt * a1\n",
        "        y2 = y[:, i] + 0.5 * dt * np.array([slip(y[0, i], v[0, i])] + [0] * (N - 1))\n",
        "        a2 = accel(u2, v2, y2, ag[i])\n",
        "        u[:, i + 1] = u[:, i] + dt * (v[:, i] + 2 * v2) / 3\n",
        "        v[:, i + 1] = v[:, i] + dt * (a1 + 2 * a2) / 3\n",
        "        y[:, i + 1] = y[:, i] + dt * (\n",
        "            np.array([slip(y[0, i], v[0, i])] + [0] * (N - 1))\n",
        "            + 2 * np.array([slip(y2[0], v2[0])] + [0] * (N - 1))\n",
        "        ) / 3\n",
        "\n",
        "    # Absolute acceleration\n",
        "    floor_acc = np.zeros((N, steps))\n",
        "    for i in range(steps):\n",
        "        udd = accel(u[:, i], v[:, i], y[:, i], ag[i])\n",
        "        floor_acc[:, i] = J @ udd + ag[i]\n",
        "\n",
        "    # Max responses\n",
        "    max_disp = np.zeros(30)\n",
        "    max_acc = np.zeros(30)\n",
        "    disp_vals = [np.max(np.abs(u[f])) for f in range(1, N)]\n",
        "    acc_vals = [np.max(np.abs(floor_acc[f])) for f in range(1, N)]\n",
        "    max_disp[:len(disp_vals)] = disp_vals\n",
        "    max_acc[:len(acc_vals)] = acc_vals\n",
        "\n",
        "    iso_disp_max = np.max(np.abs(u[0]))\n",
        "    iso_acc_max = np.max(np.abs(floor_acc[0]))\n",
        "\n",
        "    # Energy E\n",
        "    E = 0.0\n",
        "    for i in range(steps - 1):\n",
        "        du = u[0, i + 1] - u[0, i]\n",
        "        Q1 = kI * u[0, i] + kd * y[0, i]\n",
        "        Q2 = kI * u[0, i + 1] + kd * y[0, i + 1]\n",
        "        E += 0.5 * (Q1 + Q2) * du\n",
        "\n",
        "    # Output vector for ML (63)\n",
        "    Y_ml = np.concatenate([\n",
        "        max_disp,\n",
        "        max_acc,\n",
        "        [iso_disp_max],\n",
        "        [iso_acc_max],\n",
        "        [E],\n",
        "    ])\n",
        "    return Y_ml\n",
        "\n",
        "# Dataset generation\n",
        "X_list, Y_list, groups_list = [], [], []\n",
        "\n",
        "for file in os.listdir(EQ_FOLDER):\n",
        "    if not file.endswith('.txt'):\n",
        "        continue\n",
        "    eq_path = os.path.join(EQ_FOLDER, file)\n",
        "    t, ag0 = np.loadtxt(eq_path, unpack=True)\n",
        "\n",
        "    for scale in SCALE_LIST:\n",
        "        ag = ag0 * scale\n",
        "        features = extract_features(t, ag)\n",
        "\n",
        "        for W in W_LIST:\n",
        "            for floors in FLOOR_LIST:\n",
        "                for alpha in ALPHA_LIST:\n",
        "                    X = np.concatenate([[W, floors, h, alpha, scale], features])\n",
        "                    Y = run_analysis(W, floors, alpha, t, ag, h)\n",
        "\n",
        "                    X_list.append(X)\n",
        "                    Y_list.append(Y)\n",
        "                    groups_list.append(file)\n",
        "\n",
        "        print(f'Done EQ: {file} (scale={scale})')\n",
        "\n",
        "X = np.array(X_list, dtype=float)\n",
        "Y = np.array(Y_list, dtype=float)\n",
        "groups = np.array(groups_list, dtype=object)\n",
        "\n",
        "np.save(os.path.join(DATASET_FOLDER, 'X.npy'), X)\n",
        "np.save(os.path.join(DATASET_FOLDER, 'Y_ml.npy'), Y)\n",
        "np.save(os.path.join(DATASET_FOLDER, 'groups.npy'), groups)\n",
        "\n",
        "print('Dataset v2 generation complete.')\n",
        "print('X shape =', X.shape)\n",
        "print('Y_ml shape =', Y.shape)\n",
        "print('groups shape =', groups.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Scale dataset (v2)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_path = os.path.join(DATASET_FOLDER, 'X.npy')\n",
        "Y_path = os.path.join(DATASET_FOLDER, 'Y_ml.npy')\n",
        "G_path = os.path.join(DATASET_FOLDER, 'groups.npy')\n",
        "\n",
        "print('Loading dataset_v2...')\n",
        "X = np.load(X_path)\n",
        "Y = np.load(Y_path)\n",
        "groups = np.load(G_path, allow_pickle=True)\n",
        "\n",
        "assert X.shape[0] == Y.shape[0] == groups.shape[0], 'Sample count mismatch!'\n",
        "\n",
        "print('X shape:', X.shape)\n",
        "print('Y shape:', Y.shape)\n",
        "print('groups shape:', groups.shape)\n",
        "\n",
        "x_scaler = MinMaxScaler()\n",
        "y_scaler = MinMaxScaler()\n",
        "\n",
        "print('Fitting scalers...')\n",
        "X_scaled = x_scaler.fit_transform(X)\n",
        "Y_scaled = y_scaler.fit_transform(Y)\n",
        "\n",
        "np.save(os.path.join(DATASET_FOLDER, 'X_scaled.npy'), X_scaled)\n",
        "np.save(os.path.join(DATASET_FOLDER, 'Y_scaled.npy'), Y_scaled)\n",
        "\n",
        "with open(os.path.join(DATASET_FOLDER, 'x_scaler.pkl'), 'wb') as f:\n",
        "    pickle.dump(x_scaler, f)\n",
        "with open(os.path.join(DATASET_FOLDER, 'y_scaler.pkl'), 'wb') as f:\n",
        "    pickle.dump(y_scaler, f)\n",
        "\n",
        "print('Saved:')\n",
        "print(' - X_scaled.npy, Y_scaled.npy')\n",
        "print(' - x_scaler.pkl, y_scaler.pkl')\n",
        "print('[02_scale_dataset_v2 DONE]')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Train models (v2)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "X = np.load(os.path.join(DATASET_FOLDER, 'X_scaled.npy'))\n",
        "Y = np.load(os.path.join(DATASET_FOLDER, 'Y_scaled.npy'))\n",
        "groups = np.load(os.path.join(DATASET_FOLDER, 'groups.npy'), allow_pickle=True)\n",
        "\n",
        "input_dim = X.shape[1]\n",
        "output_dim = Y.shape[1]\n",
        "\n",
        "print('X shape=', X.shape, 'Y shape=', Y.shape, 'output_dim=', output_dim)\n",
        "\n",
        "def build_model(input_dim, output_dim):\n",
        "    inp = layers.Input(shape=(input_dim,))\n",
        "    x = layers.Dense(128)(inp)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Dropout(0.15)(x)\n",
        "\n",
        "    x = layers.Dense(256)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Dropout(0.15)(x)\n",
        "\n",
        "    x = layers.Dense(128)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    out = layers.Dense(output_dim, activation='linear')(x)\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=tf.keras.losses.Huber(delta=1.0),\n",
        "        metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def train_one_floor(floor_value):\n",
        "    # Filter by raw floors (before scaling)\n",
        "    X_raw = np.load(os.path.join(DATASET_FOLDER, 'X.npy'))\n",
        "    floors_raw = X_raw[:, 1].astype(int)\n",
        "    idx = np.where(floors_raw == floor_value)[0]\n",
        "\n",
        "    Xf = X[idx]\n",
        "    Yf = Y[idx]\n",
        "    gf = groups[idx]\n",
        "\n",
        "    print(f'\n=== Training for floors={floor_value} ===')\n",
        "    print('samples =', len(idx))\n",
        "\n",
        "    splitter = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n",
        "    train_idx, val_idx = next(splitter.split(Xf, Yf, groups=gf))\n",
        "\n",
        "    X_train, Y_train = Xf[train_idx], Yf[train_idx]\n",
        "    X_val, Y_val = Xf[val_idx], Yf[val_idx]\n",
        "\n",
        "    model = build_model(input_dim, output_dim)\n",
        "    ckpt_path = os.path.join(MODEL_FOLDER, f'model_f{floor_value}.keras')\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(ckpt_path, monitor='val_loss', save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-5),\n",
        "        EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True),\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        X_train, Y_train,\n",
        "        validation_data=(X_val, Y_val),\n",
        "        epochs=400,\n",
        "        batch_size=32,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    final_path = os.path.join(MODEL_FOLDER, f'model_f{floor_value}_final.keras')\n",
        "    model.save(final_path)\n",
        "    print('Saved best: ', ckpt_path)\n",
        "    print('Saved final:', final_path)\n",
        "\n",
        "for f in [5, 10, 20, 30]:\n",
        "    train_one_floor(f)\n",
        "\n",
        "print('All models trained.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Predict (v2)\n",
        "Update the parameters in the cell below, then run.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# User inputs\n",
        "W_total = 8437.5\n",
        "floors = 10  # 5, 10, 20, 30\n",
        "h = 0.02\n",
        "alpha = 0.05\n",
        "scale = 1.0\n",
        "eq_path = os.path.join(EQ_FOLDER, 'hoei-kobe.txt')  # change to your file\n",
        "\n",
        "# Load scalers\n",
        "with open(os.path.join(DATASET_FOLDER, 'x_scaler.pkl'), 'rb') as f:\n",
        "    x_scaler = pickle.load(f)\n",
        "with open(os.path.join(DATASET_FOLDER, 'y_scaler.pkl'), 'rb') as f:\n",
        "    y_scaler = pickle.load(f)\n",
        "\n",
        "Dp = 354.0\n",
        "H = 523.0\n",
        "g = 980.0\n",
        "\n",
        "def trapezoid(y, dx):\n",
        "    return np.trapezoid(y, dx=dx) if hasattr(np, 'trapezoid') else np.trapz(y, dx=dx)\n",
        "\n",
        "def calc_r(Dp):\n",
        "    return 0.001214 * Dp + 0.5698 if Dp <= 354.0 else 1.0\n",
        "\n",
        "def calc_rho(E_Nmm, Vp, Dp):\n",
        "    r = calc_r(Dp)\n",
        "    E_over_Vp = E_Nmm / Vp\n",
        "    E_eff_over_Vp = r * E_over_Vp\n",
        "    rho = (8.33 / 7.967) * (-0.06 + 1.25 * np.exp(-E_eff_over_Vp / 360.0))\n",
        "    return min(rho, 1.0)\n",
        "\n",
        "def extract_features(t, ag):\n",
        "    dt = t[1] - t[0]\n",
        "    PGA = np.max(np.abs(ag))\n",
        "    PGV = np.max(np.abs(np.cumsum(ag) * dt))\n",
        "    PGD = np.max(np.abs(np.cumsum(np.cumsum(ag) * dt) * dt))\n",
        "    RMS = np.sqrt(np.mean(ag ** 2))\n",
        "    IA = trapezoid(ag ** 2, dx=dt)\n",
        "    CAV = np.sum(np.abs(ag)) * dt\n",
        "    duration = t[-1] - t[0]\n",
        "    return np.array([PGA, PGV, PGD, RMS, IA, CAV, duration], dtype=float)\n",
        "\n",
        "if floors not in [5, 10, 20, 30]:\n",
        "    raise ValueError('floors must be 5, 10, 20, or 30')\n",
        "if not os.path.exists(eq_path):\n",
        "    raise FileNotFoundError('Earthquake file not found: ' + eq_path)\n",
        "\n",
        "t, ag0 = np.loadtxt(eq_path, unpack=True)\n",
        "ag = ag0 * scale\n",
        "\n",
        "features = extract_features(t, ag)\n",
        "X = np.concatenate([[W_total, floors, h, alpha, scale], features]).reshape(1, -1)\n",
        "X_scaled = x_scaler.transform(X)\n",
        "\n",
        "model_path = os.path.join(MODEL_FOLDER, f'model_f{floors}.keras')\n",
        "model = tf.keras.models.load_model(model_path, compile=False)\n",
        "print('Loaded model:', model_path)\n",
        "\n",
        "Y_scaled = model.predict(X_scaled, verbose=0)\n",
        "Y = y_scaler.inverse_transform(Y_scaled)[0]\n",
        "\n",
        "max_disp = Y[0:30]\n",
        "max_acc  = Y[30:60]\n",
        "iso_disp = Y[60]\n",
        "iso_acc  = Y[61]\n",
        "E        = Y[62]\n",
        "\n",
        "SigmaW = W_total * 1000.0\n",
        "Vp = alpha * SigmaW * H / 8.33\n",
        "E_Nmm = E * 1e4\n",
        "Evp = E_Nmm / Vp\n",
        "rho = calc_rho(E_Nmm, Vp, Dp)\n",
        "\n",
        "print('\\n=== ML PREDICTION RESULT (v2) ===')\n",
        "print('\\n--- Floor Maximum Displacement [cm] ---')\n",
        "for i in range(floors):\n",
        "    print(f'Floor {i+1:2d}: {max_disp[i]:.5f}')\n",
        "\n",
        "print('\\n--- Floor Maximum Absolute Acceleration [cm/s^2] ---')\n",
        "for i in range(floors):\n",
        "    print(f'Floor {i+1:2d}: {max_acc[i]:.5f}')\n",
        "\n",
        "print('\\n--- Isolator Response ---')\n",
        "print(f'Isolator max displacement = {iso_disp:.5f} cm')\n",
        "print(f'Isolator max abs accel    = {iso_acc:.5f} cm/s^2')\n",
        "\n",
        "print('\\n--- Energy & Degradation Indices (Hybrid Physics) ---')\n",
        "print(f'E      = {E:.6e} kN*cm')\n",
        "print(f'Vp     = {Vp:.6e} (computed)')\n",
        "print(f'E / Vp = {Evp:.6e} (computed)')\n",
        "print(f'rho    = {rho:.6f} (computed)')\n",
        "\n",
        "print('\\nPrediction complete.')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "earthquake_ml_pipeline_v2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}